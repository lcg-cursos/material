{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prompting Cient\u00edfico \u2014 Caso MalT (Protocolo Completo)\n",
        "\n",
        "Este notebook implementa un flujo multi-etapa con control epistemol\u00f3gico distribuido.\n",
        "Ejecuta cada fase secuencialmente y realiza revisi\u00f3n humana en los checkpoints indicados.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip -q install --upgrade openai"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os, json, datetime\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI()\n",
        "\n",
        "models = client.models.list()\n",
        "available = [m.id for m in models.data]\n",
        "MODEL = available[0]\n",
        "MODEL"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def response_to_text(resp):\n",
        "    chunks = []\n",
        "    if hasattr(resp, 'output') and resp.output:\n",
        "        for item in resp.output:\n",
        "            if hasattr(item, 'content') and item.content:\n",
        "                for c in item.content:\n",
        "                    if getattr(c, 'type', None) == 'output_text':\n",
        "                        chunks.append(getattr(c, 'text', ''))\n",
        "    return '\\n'.join(chunks).strip()\n",
        "\n",
        "def call_llm(prompt, system=None):\n",
        "    messages = []\n",
        "    if system:\n",
        "        messages.append({'role': 'system', 'content': system})\n",
        "    messages.append({'role': 'user', 'content': prompt})\n",
        "    resp = client.responses.create(model=MODEL, input=messages)\n",
        "    return response_to_text(resp)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pegar aqu\u00ed el texto del paper\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "paper_text = '''\n",
        "PEGA AQUI EL TEXTO DEL PAPER\n",
        "'''\n",
        "paper_text = paper_text.strip()\n",
        "len(paper_text)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fase 1 \u2014 Exploraci\u00f3n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "PROMPT_F1 = f'''\n",
        "Rol: Explorador cient\u00edfico.\n",
        "Identifica p\u00e1rrafos que mencionen MalT y evidencia regulatoria.\n",
        "No interpretar. No resumir. Citar literalmente.\n",
        "\n",
        "Texto:\n",
        "{paper_text}\n",
        "'''\n",
        "\n",
        "f1_out = call_llm(PROMPT_F1)\n",
        "print(f1_out[:2000])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fase 2 \u2014 Extracci\u00f3n literal (Checkpoint humano antes de ejecutar)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "PROMPT_F2 = f'''\n",
        "Extrae \u00fanicamente informaci\u00f3n expl\u00edcita sobre regulaci\u00f3n mediada por MalT.\n",
        "No inferir. Incluir cita textual.\n",
        "\n",
        "Fragmentos:\n",
        "{f1_out}\n",
        "'''\n",
        "\n",
        "f2_out = call_llm(PROMPT_F2)\n",
        "print(f2_out[:2000])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fase 3 \u2014 Normalizaci\u00f3n\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "PROMPT_F3 = f'''\n",
        "Estandariza nombres g\u00e9nicos. Reporta ambig\u00fcedades.\n",
        "Tabla de entrada:\n",
        "{f2_out}\n",
        "'''\n",
        "\n",
        "f3_out = call_llm(PROMPT_F3)\n",
        "print(f3_out[:2000])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fase 4 \u2014 Evaluaci\u00f3n cr\u00edtica (Revisi\u00f3n humana obligatoria)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "PROMPT_F4 = f'''\n",
        "Clasifica evidencia como Directa, Indirecta o Correlacional.\n",
        "Define criterios expl\u00edcitos y justifica con cita textual.\n",
        "\n",
        "Tabla:\n",
        "{f3_out}\n",
        "'''\n",
        "\n",
        "f4_out = call_llm(PROMPT_F4)\n",
        "print(f4_out[:2000])"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fase 5 \u2014 Integraci\u00f3n humana\n",
        "\n",
        "El investigador integra, valida y decide.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}